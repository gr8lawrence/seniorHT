---
title: "Tianyi's Senior Honors Thesis"
author: "Tianyi Liu" 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(parallel)
library(ranger) # fast implementation of random forest
library(e1071) # svm
library(RMTL) # Multitask learning in R
```

## Datasets

Five datasets containing RNA-seq data from different clinical trials are included in our study:

* Aguirre-seq
* COMPASS
* Linehan-seq 
* Moffitt-GEO-array
* TCGA-PAAD

***
#### Seeding
We first set the randomization seed to make this study reproducible
``` {r setSeed}
set.seed(100)
```

## Data Pre-processing
### Loading and subsetting
We can easily load the datasets to a list object and make it convenient for adding new or changing datasets.
```{r loadData, echo = FALSE}
dataPath <-
  "/Users/gr8lawrence/Desktop/Senior Honors Thesis/datasets/"

studies.names <-
  c("Aguirre-Seq",
    "Linehan-Seq",
    "COMPASS",
    "Moffitt Arrays",
    "TCGA-PAAD") # Names of the studies in the same order as loading below

# Load datasets
load(paste(dataPath, "Aguirre_seq_plus.RData", sep = ""))
load(paste(dataPath, "Linehan_Seq_plus.RData", sep = ""))
load(paste(dataPath, "COMPASS.2017_plus.RData", sep = ""))
load(paste(dataPath, "Moffitt_GEO_array_plus.RData", sep = ""))
load(paste(dataPath, "TCGA_PAAD_plus.RData", sep = ""))

# Incorporate all datasets into a single list object
studies.df <-
  list(
    Aguirre_seq_plus,
    Linehan_Seq_plus,
    COMPASS.2017_plus,
    Moffitt_GEO_array_plus,
    TCGA_PAAD_plus
  )

num.studies <- length(studies.df) # Number of datasets
```
Here, we first subset the datasets so they only contain common genes. After we load the datasets, we sort the them by `SYMBOL` of genes in alphanumeric order. Then we find out the gene names all datasets have in common and store them in the vector ``
```{r sortGene}
# Sort gene in alphanumeric order
geneSort <- function(d) {
  # d is a dataset
  index <- order(d$featInfo$SYMBOL)
  d$ex[,] <- d$ex[index,]
  d$featInfo[,] <- d$featInfo[index,]
  return(d)
}

for (i in 1:num.studies) {
  studies.df[[i]] <- geneSort(studies.df[[i]])
}

# Obtain the list of gene symbols from datasets
getGeneSymbols <- function(d) {
  symbols <- d$featInfo$SYMBOL
  return(symbols)
} 

# Find the common genes across all datasets
commonGeneNames <- getGeneSymbols(studies.df[[1]])
for (i in 2:num.studies) {
  commonGeneNames <- intersect(commonGeneNames, getGeneSymbols(studies.df[[i]]))
}

num.common.genes <- length(commonGeneNames) # Number of common genes
```
We can see the number of common genes is `r num.common.genes`. Next, we subset our datasets so they only contain those common genes.
``` {r subsetting}
# Subset the datasets to ones of common genes
matchCommonGenes <- function(d) {
  index <- match(commonGeneNames, d$featInfo$SYMBOL)
  d$ex <- d$ex[index, ]
  d$featInfo <- d$featInfo[index, ]
  return(d)
}

for (i in 1:num.studies) {
  studies.df[[i]] <- matchCommonGenes(studies.df[[i]])
}
```

### Rank transformation
Because the measurements of RNA transcription abundance per sample are different across clinical trials, we rank transoform the columns of our data to unify the measurements.
``` {r rankTransform}
# Rank transform the common genes for each sample
rankTransform <- function(d) {
  for (i in 1:dim(d$ex)[2]) {
    d$ex[,i] <- rank(d$ex[,i])
  }
  return(d)
}

ranked.studies.df <- list()
for (i in 1:num.studies) {
  ranked.studies.df[[i]] <- rankTransform(studies.df[[i]])
}
```

### Dimension Reduction
In this part, we reduce the dimensionality of the datasets to save computational time and power by including only the highly differentially expressed genes between two identified cancer subtypes. We follow the following procedures to achieve this.

#### Finding the differentially expressed genes in basal pancreatic cancer
We first extract the ranks of gene expressions from our datasets and remove the observations with unknown classification (`NA` in `cluster.MT`). 
``` {r extractGeneExpression}
# Extract ranked expression from each dataset, remove observations of unknown cancer classification, and write them into tibbles
extractData <- function(d) {
  df <- as_tibble(t(d$ex))
  colnames(df) <- make.names(commonGeneNames) # Get all gene names into correct name formats
  df <- df[!is.na(d$sampInfo$cluster.MT),] # Remove observations with unknown subtype
  return(add_column(df, class = d$sampInfo$cluster.MT[!is.na(d$sampInfo$cluster.MT)]))
}

expression.df <- list()
for (i in 1:num.studies) {
  expression.df[[i]] <- extractData(ranked.studies.df[[i]])
}
```
Then we separate the each expression dataset into two subsets of observations based on their `cluster.MT` value (either `basal` or `classical`). 
``` {r subsettingExpressionData}
expression.basal.df <- list()
expression.classical.df <- list()
for (i in 1:num.studies) {
  expression.basal.df[[i]] <- expression.df[[i]][expression.df[[i]]$class == "basal", ]
  expression.classical.df[[i]] <- expression.df[[i]][expression.df[[i]]$class == "classical", ]
}
```
Then we conduct a Wilcoxon Rank Sum test on each gene for each dataset, and then sum up each gene's log-10 p-values to find out the consistently differentially expressed genes in the `basal` subtype across all studies.
``` {r wilcoxTestForDiffExpressedGenes}
# Transforming dataset to matrices so we can conduct statistical tests (Wilcoxon rank sum test)
basal.matrices <- list()
classical.matrices <- list()

for (i in 1:num.studies) {
  basal.matrices[[i]] <- data.matrix(expression.basal.df[[i]][ ,1:num.common.genes])
  classical.matrices[[i]] <- data.matrix(expression.classical.df[[i]][ ,1:num.common.genes])
}

# Build up empty list to contain the p-values of single tests (in one dataset) for each gene
gene.pvals <- list()
for (i in 1:num.studies){
  gene.pvals[[i]] <- tibble(name = make.names(commonGeneNames),
                     pval = rep(0, num.common.genes))
}

for (i in 1:num.studies) {
  for (j in 1:num.common.genes) {
    basal.vector <- basal.matrices[[i]][ ,j]
    classical.vector <- classical.matrices[[i]][ ,j]
    gene.pvals[[i]]$pval[j] <- wilcox.test(basal.vector, classical.vector, exact = FALSE)[["p.value"]]
  }
}

# Compute each gene's sum of log-10 p-values
gene.sum.log.pvals <- tibble(name = make.names(commonGeneNames),
                     pval = rep(0, length(commonGeneNames)))

for (i in 1:num.common.genes) {
  gene.pvals.vector <- vector()
  for (j in 1:num.studies) {
    gene.pvals.vector[j] = gene.pvals[[j]]$pval[i]
  }
  log.10.gene.pvals <- log10(gene.pvals.vector)
  gene.sum.log.pvals$pval[i] <- sum(log.10.gene.pvals)
}

gene.sum.log.pvals
```
Now we rank the genes in terms of their p-values. 
``` {r sortGene2}
index <- order(gene.sum.log.pvals$pval) # Give the sorted indexes of genes

reorderGene <- function(df) {
  df.colnames <- colnames(df)
  df[ ,1:length(commonGeneNames)] <- data.matrix(df[ ,index])
  colnames(df) <- df.colnames[index]
  return(as_tibble(df))
}

for (i in 1:num.studies) {
  expression.df[[i]] <- reorderGene(expression.df[[i]])  
}
```
Then we subset the datasets to keep only the first `r ceiling(length(commonGeneNames)/5)` most differentially expressed genes, and append the `cluster.MT` of the observations at the end to prepare for learning that is coming next.
``` {r subsetting2}
df.length <- ceiling(length(commonGeneNames)/5)

learning.df <- list()
for (i in 1:num.studies) {
 learning.df[[i]] <- add_column(expression.df[[i]][ , 1:df.length], 
                                class = ranked.studies.df[[i]]$sampInfo$cluster.MT[!is.na(ranked.studies.df[[i]]$sampInfo$cluster.MT)]) 
}
```

***
## Learning From Data
### Learning by vanilla RF and SVM
#### Overview
We apply both standard implementations of RF (from `ranger` package) and SVM (from `e1071` package) to this learning problem. First, we choose a dataset and designate it as the reference set, or the "truth" set. Then we train on the remaining datasets ("learning sets") in two ways: training on a single learning set, or on a combined dataset consists of all learning sets. After each training, we test its predictive power on the reference set by the following measurements: (prediction) accuracy, sensitivity and specificity.

#### Run the learning programs
Now we run the RF and SVM in all combinations possible and output the measurements reflecting the larning results, i.e. accuracy, sensitivity, specificity.
``` {r learning1}
studies <- learning.df

# We create an empty tibble first to hold the leanring results
learning.results <- tibble(learning.set = rep('NA', 2*length(studies)^2), 
                           validation.set = rep('NA', 2*length(studies)^2), 
                           method = rep('NA', 2*length(studies)^2),
                           accuracy = rep(0, 2*length(studies)^2), 
                           sensitivity = rep(0, 2*length(studies)^2), 
                           specificity = rep(0, 2*length(studies)^2))
len <- length(studies)
  
for (i in 1:len) {
  studies.min.1 <- studies[-i]
  studies.names.min.1 <- studies.names[-i]
  validation.set <- studies[[i]][, 1:df.length]
  truth <- studies[[i]]$class # The truth vector
  
  # Run the RF (with 1500 trees) and SVM on one dataset, and validate on studies[i]
  for (j in 1:length(studies.min.1)) {
    learning.set <- studies.min.1[[j]]
    
    # Random Forest
    rf <-
      ranger::ranger(class ~ ., data = learning.set, num.trees = 1500, probability = TRUE)
    pred <- predict(rf, validation.set)
    confusion.mat <- confusionMatrix(data = as.factor(ifelse(pred$predictions[,1] > 0.5, "basal", "classical")),
                                     reference = truth)
    accu <- confusion.mat$overall[["Accuracy"]]
    sen <- confusion.mat$byClass[["Sensitivity"]]
    spe <- confusion.mat$byClass[["Specificity"]]
    learning.results[10 * (i - 1) + 2 * j - 1,] <-
      c(
        studies.names.min.1[j],
        studies.names[i],
        "random forest",
        signif(accu, digits = 4),
        signif(sen, digits = 4),
        signif(spe, digits = 4)
      )
    
    # Supporting Vector Machine
    classical.basal.ratio = sum(learning.set$class == "classical")/sum(learning.set$class == "basal")
    supp.vec <-
    svm(class ~ ., data = learning.set, class.weights = c("basal" = classical.basal.ratio, "classical" = 0.1), probability = TRUE)
    pred <- predict(supp.vec, validation.set, probability = TRUE)
    confusion.mat <- confusionMatrix(data = pred,
                                     reference = truth)
    accu <- confusion.mat$overall[["Accuracy"]]
    sen <- confusion.mat$byClass[["Sensitivity"]]
    spe <- confusion.mat$byClass[["Specificity"]]
    learning.results[10 * (i - 1) + 2 * j,] <-
      c(
        studies.names.min.1[j],
        studies.names[i],
        "SVM",
        signif(accu, digits = 4),
        signif(sen, digits = 4),
        signif(spe, digits = 4)
      )
  }
  
  # Run the RF and SVM on combined datasets
  learning.set <- studies.min.1[[1]]
  for (k in 1:(length(studies.min.1) - 1)) {
    learning.set <- rbind(learning.set, studies.min.1[[1 + k]])
  }
  
  # Random Forest
  rf <-
    ranger::ranger(class ~ ., data = learning.set, num.trees = 1500, probability = TRUE)
  pred <- predict(rf, validation.set)
  confusion.mat <- confusionMatrix(data = as.factor(ifelse(pred$predictions[,1] > 0.5, "basal", "classical")), 
                                   reference = truth)
  accu <- confusion.mat$overall[["Accuracy"]]
  sen <- confusion.mat$byClass[["Sensitivity"]]
  spe <- confusion.mat$byClass[["Specificity"]]
  learning.results[10 * i - 1,] <-
    c(
      paste("comb. minus", studies.names[i],
            sep = " "),
      studies.names[i],
      "random forest",
      signif(accu, digits = 4),
      signif(sen, digits = 4),
      signif(spe, digits = 4)
    )
  
  # Supporting Vector Machine
  classical.basal.ratio = sum(learning.set$class == "classical")/sum(learning.set$class == "basal")
  supp.vec <-
    svm(class ~ ., data = learning.set, class.weights = c("basal" = classical.basal.ratio, "classical" = 1), probability = TRUE)
  pred <- predict(supp.vec, validation.set, probability = TRUE)
  confusion.mat <- confusionMatrix(data = pred,
                                   reference = truth)
  accu <- confusion.mat$overall[["Accuracy"]]
  sen <- confusion.mat$byClass[["Sensitivity"]]
  spe <- confusion.mat$byClass[["Specificity"]]
  learning.results[10 * i,] <-
    c(
      paste("comb. minus", studies.names[i],
            sep = " "),
      studies.names[i],
      "SVM",
      signif(accu, digits = 4),
      signif(sen, digits = 4),
      signif(spe, digits = 4)
    )
  
}

print(learning.results, n = 2*length(studies)^2)
```
#### Visualization of results
We draw a couple ROC curves based on the comparison of learning results above.
``` {r ROC curve}
# Convert characters to doubles
roc.data <- learning.results
roc.data$accuracy <- as.double(roc.data$accuracy) 
roc.data$sensitivity <- as.double(roc.data$sensitivity)
roc.data$specificity <- as.double(roc.data$specificity)

# Add a column for learning set
learning.type <- c(rep("single", 2*(length(studies) - 1)), rep("combined", 2))
learning.type <- rep(learning.type, 5)
learning.type <- t(t(learning.type))
learning.type <- factor(learning.type, levels = c("single", "combined"))
roc.data <- add_column(roc.data, Type = as.vector(learning.type))

# Add a column for false positive 
roc.data <- add_column(roc.data, false.positive = rep(1, length(roc.data$specificity)))
roc.data$false.positive <- roc.data$false.positive - roc.data$specificity

# Separate validation set type
is.Moffitt <- roc.data$validation.set == "Moffitt Arrays"
roc.data <- add_column(roc.data, validation.set.type = rep("NGS", length(roc.data$specificity)))
roc.data$validation.set.type[is.Moffitt] <- "microarrays"
roc.data$validation.set.type <- factor(roc.data$validation.set.type, levels = c("NGS", "microarrays"))

# Plot the roc curves
ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set, shape = method)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Methods") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid(validation.set ~ method, scales = "free") +
  scale_color_discrete(name = "Validation Dataset") +
  scale_shape_discrete(name = "Learning Method")

ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Types of Training Sets") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid(validation.set ~ Type, scales = "free") +
  scale_color_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Learning Methods Types of Studies of Validation Sets") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid( Type ~ validation.set.type) +
  scale_color_discrete(name = "Validation Dataset")

# Plot out the accuracy
ggplot(data = roc.data, aes(x = Type, y = accuracy, fill = validation.set)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Training Sets") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid( . ~ validation.set) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 15)) +
  scale_fill_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = Type, y = accuracy, fill = validation.set)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Training Sets And Learning Methods") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid(method ~ validation.set) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 15)) +
  scale_fill_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = validation.set.type, y = accuracy, fill = validation.set.type)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Studies of Validation Sets") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid( . ~ Type) +
  theme(axis.text.x = element_text(vjust = 0.5)) +
  scale_fill_discrete(name = "Type of Studies")
```

### Multi-task Learning
In this section, we explore our datasets with learning programs from the package with implementations of multi-task learning `RMTL` in R.