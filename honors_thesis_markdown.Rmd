---
title: "Tianyi's Senior Honors Thesis"
author: "Tianyi Liu" 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(parallel)
library(ranger) # fast implementation of random forest
library(e1071) # svm
library(RMTL) # Multitask learning in R
```

## Datasets

Five datasets containing RNA-seq data from different clinical trials are included in our study:

* Aguirre-seq
* COMPASS
* Linehan-seq 
* Moffitt-GEO-array
* TCGA-PAAD

***

## Data Pre-processing
### Loading and subsetting
```{r loadData, echo = FALSE}
dataPath <- "/Users/gr8lawrence/Desktop/Senior Honors Thesis/datasets/"
load(paste(dataPath, "Aguirre_seq_plus.RData", sep = ""))
load(paste(dataPath, "COMPASS.2017_plus.RData", sep = ""))
load(paste(dataPath, "Linehan_Seq_plus.RData", sep = ""))
load(paste(dataPath, "Moffitt_GEO_array_plus.RData", sep = ""))
load(paste(dataPath, "TCGA_PAAD_plus.RData", sep = ""))
```

We first set the randomization seed
``` {r setSeed}
set.seed(100)
```

Our first goal is to subset the datasets so they only contain common genes. After we load the datasets, we sort the them by `SYMBOL` of genes in alphanumeric order.
```{r sortGene}
# Sort gene in alphanumeric order
geneSort <- function(d) { # d is a dataset
  index <- order(d$featInfo$SYMBOL)
  d$ex[ , ] <- d$ex[index, ] 
  d$featInfo[ , ] <- d$featInfo[index, ]
  return(d)
}

Aguirre_seq_plus <- geneSort(Aguirre_seq_plus)
COMPASS_plus <- geneSort(COMPASS.2017_plus)
Linehan_Seq_plus <- geneSort(Linehan_Seq_plus)
Moffitt_GEO_array_plus <- geneSort(Moffitt_GEO_array_plus)
TCGA_PAAD_plus <- geneSort(TCGA_PAAD_plus)
```

After we called the above functions on all five datasets, we need to find the common genes across them by intersecting their `SYMBOL`.

``` {r findCommonGene}
# Get the list of gene symbols from datasets
getGeneSymbols <- function(d) {
  symbols <- d$featInfo$SYMBOL
  return(symbols)
} 

commonNames1 <- intersect(getGeneSymbols(Linehan_Seq_plus), getGeneSymbols(COMPASS.2017_plus))
commonNames2 <- intersect(getGeneSymbols(Moffitt_GEO_array_plus), getGeneSymbols(TCGA_PAAD_plus))
commonNames3 <- intersect(commonNames1, commonNames2)
commonNames <- intersect(commonNames3, getGeneSymbols(Aguirre_seq_plus))
```

We can see the number of common genes is `r length(commonNames)`.  
Next, we subset our datasets so they only contain those common genes.
``` {r subsetting}
# Subset the datasets to ones of common genes
matchCommonGenes <- function(d) {
  index <- match(commonNames, d$featInfo$SYMBOL)
  d$ex <- d$ex[index, ]
  d$featInfo <- d$featInfo[index, ]
  return(d)
}

Agu <- matchCommonGenes(Aguirre_seq_plus)
Lin <- matchCommonGenes(Linehan_Seq_plus) 
Com <- matchCommonGenes(COMPASS.2017_plus)
Mof <- matchCommonGenes(Moffitt_GEO_array_plus)
Tcga <- matchCommonGenes(TCGA_PAAD_plus)

# We extract the names of the common genes fo future use
geneNames <- Agu$featInfo$SYMBOL
```

### Rank transformation
Because the measurements of RNA transcription abundance per sample are different across clinical trials, we rank transoform the columns of our data to unify the measurements.
``` {r rankTransform}
# Rank transform the common genes for each sample
rankTransform <- function(d) {
  for (i in 1:dim(d$ex)[2]) {
    d$ex[,i] <- rank(d$ex[,i])
  }
  return(d)
}

rankedAgu <- rankTransform(Agu)
rankedLin <- rankTransform(Lin)
rankedCom <- rankTransform(Com)
rankedMof <- rankTransform(Mof)
rankedTcga <- rankTransform(Tcga)
```

### Finding the differentially expressed in basal pancreatic cancer
In order to do so, we first need to pool the five datasets and remove the observations with unknown classification (`NA` in `cluster.MT`). 
``` {r poolDataset}
# Extract ranked expression from each dataset, remove observations of unknown cancer classification, and write them into tibbles
extractData <- function(d) {
  df <- as_tibble(t(d$ex))
  colnames(df) <- make.names(geneNames) # Get all gene names into correct name formats
  df <- df[!is.na(d$sampInfo$cluster.MT), ]
  return(add_column(df, cluster = d$sampInfo$cluster.MT[!is.na(d$sampInfo$cluster.MT)]))
}

dfAgu <- extractData(rankedAgu)
dfLin <- extractData(rankedLin)
dfCom <- extractData(rankedCom) 
dfMof <- extractData(rankedMof)
dfTcga <- extractData(rankedTcga)

# Combined the five (four) expression datasets
pooled.df <- rbind(dfAgu, dfLin, dfCom, dfMof, dfTcga)
```
Then we separate the pooled datasets into two subsets of observations based on their `cluster.MT` value (either `basal` or `classical`). 
``` {r subsettingPooledData}
pooled.basal <-pooled.df[pooled.df$cluster == "basal", ]
pooled.classical <-pooled.df[pooled.df$cluster == "classical", ]
```
Then we conduct a Wilcoxon Rank Sum test for each gene to find out the consistently differentially expressed genes in the `basal` subtype across all studies.
``` {r wilcoxTestForDiffExpressedGenes}
basal.matrix <- data.matrix(pooled.basal[ ,1:length(geneNames)])
classical.matrix <- data.matrix(pooled.classical[ ,1:length(geneNames)])
gene.pvals <- tibble(name = make.names(geneNames),
                     pval = rep(0, length(geneNames)))
  
for (i in 1:length(geneNames)) {
  basal.vector <- basal.matrix[ ,i]
  classical.vector <- classical.matrix[ ,i]
  gene.pvals$pval[i] <- wilcox.test(basal.vector, classical.vector)[["p.value"]]
}

gene.pvals
```
Now we rank the genes in terms of their p-values. 
``` {r sortGene2}
index <- order(gene.pvals$pval) # Give the sorted indexes of genes

reorderGene <- function(df) {
  df.colnames <- colnames(df)
  df[ ,1:length(geneNames)] <- data.matrix(df[ ,index])
  colnames(df) <- df.colnames[index]
  return(as_tibble(df))
}

dfAgu <- reorderGene(dfAgu)
dfLin <- reorderGene(dfLin)
dfCom <- reorderGene(dfCom)
dfMof <- reorderGene(dfMof)
dfTcga <- reorderGene(dfTcga)
```
Then we subset the datasets to keep only the first `r ceiling(length(geneNames)/10)` most differentially expressed genes.
``` {r subsetting2}
df.length <- ceiling(length(geneNames)/10)
dfAgu <- dfAgu[ ,1:df.length]
dfLin <- dfLin[ ,1:df.length]
dfCom <- dfCom[ ,1:df.length]
dfMof <- dfMof[ ,1:df.length]
dfTcga <- dfTcga[ ,1:df.length]
```

***
## Learning From Data
### Learning by vanilla RF and SVM
#### Overview
We apply both standard implementations of RF (from `ranger` package) and SVM (from `e1071` package) to this learning problem. First, we choose a dataset and designate it as the reference set, or the "truth" set. Then we train on the remaining datasets ("learning sets") in two ways: training on a single learning set, or on a combined dataset consists of all learning sets. After each training, we test its predictive power on the reference set by the following measurements: (prediction) accuracy, sensitivity and specificity.

#### Data Processing
In order to proceed conveniently, we put all studies into a single vector, so we can run the learning programs iteratively through a for loop. First, we prepare the data
``` {r mlPrep}
# Append the cluster result to each dataset
dfAgu <- add_column(dfAgu, 
                    Cluster = rankedAgu$sampInfo$cluster.MT[!is.na(rankedAgu$sampInfo$cluster.MT)])

dfLin <- add_column(dfLin, 
                    Cluster = rankedLin$sampInfo$cluster.MT[!is.na(rankedLin$sampInfo$cluster.MT)])

dfCom <- add_column(dfCom, 
                    Cluster = rankedCom$sampInfo$cluster.MT[!is.na(rankedCom$sampInfo$cluster.MT)])

dfMof <- add_column(dfMof, 
                    Cluster = rankedMof$sampInfo$cluster.MT[!is.na(rankedMof$sampInfo$cluster.MT)])

dfTcga <- add_column(dfTcga, 
                    Cluster = rankedTcga$sampInfo$cluster.MT[!is.na(rankedTcga$sampInfo$cluster.MT)])

# The vector of all studies
studies <- list(dfAgu, dfLin, dfCom, dfTcga, dfMof) 
studies.names <- c("Aguirre-Seq", "Linehan-Seq", "COMPASS", "Moffitt Arrays", "TCGA-PAAD")
```
#### Run the learning programs
Now we run the RF and SVM in all combinations possible and output the measurements reflecting the larning results, i.e. accuracy, sensitivity, specificity.
``` {r learning1}
# We create an empty tibble first to hold the leanring results
learning.results <- tibble(learning.set = rep('NA', 2*length(studies)^2), 
                           validation.set = rep('NA', 2*length(studies)^2), 
                           method = rep('NA', 2*length(studies)^2),
                           accuracy = rep(0, 2*length(studies)^2), 
                           sensitivity = rep(0, 2*length(studies)^2), 
                           specificity = rep(0, 2*length(studies)^2))
len <- length(studies)
  
for (i in 1:len) {
  studies.min.1 <- studies[-i]
  studies.names.min.1 <- studies.names[-i]
  validation.set <- studies[[i]][ ,1:df.length]
  truth <- studies[[i]]$Cluster # The truth vector
  
  # Run the RF (with 1500 trees) and SVM on one dataset, and validate on studies[i]
  for (j in 1:length(studies.min.1)) {
    learning.set <- studies.min.1[[j]]
    
    # Random Forest
    rf <- ranger::ranger(Cluster~., data = learning.set, num.trees = 1500)
    pred <- predict(rf, validation.set)
    confusion.mat <- confusionMatrix(data = pred$predictions, 
                                     reference = truth)
    accu <- confusion.mat$overall[["Accuracy"]]
    sen <- confusion.mat$byClass[["Sensitivity"]]
    spe <- confusion.mat$byClass[["Specificity"]]
    learning.results[10*(i - 1) + 2*j - 1, ] <- c(studies.names.min.1[j],
                                            studies.names[i],
                                            "random forest",
                                            signif(accu, digits = 4), 
                                            signif(sen, digits = 4), 
                                            signif(spe, digits = 4))
    
    # Supporting Vector Machine
    supp.vec <- svm(Cluster~., data = learning.set)
    pred <- predict(supp.vec, validation.set)
    confusion.mat <- confusionMatrix(data = pred, 
                                     reference = truth)
    accu <- confusion.mat$overall[["Accuracy"]]
    sen <- confusion.mat$byClass[["Sensitivity"]]
    spe <- confusion.mat$byClass[["Specificity"]]
    learning.results[10*(i - 1) + 2*j, ] <- c(studies.names.min.1[j],
                                            studies.names[i],
                                            "SVM",
                                            signif(accu, digits = 4), 
                                            signif(sen, digits = 4), 
                                            signif(spe, digits = 4))
  }
  
  # Run the RF and SVM on combined datasets
  learning.set <- studies.min.1[[1]]
  for (k in 1:(length(studies.min.1)-1)){
  learning.set <- rbind(learning.set, studies.min.1[[1 + k]])
  }
  
  # Random Forest
  rf <- ranger::ranger(Cluster~., data = learning.set, num.trees = 1500)
  pred <- predict(rf, validation.set)
  confusion.mat <- confusionMatrix(data = pred$predictions, 
                                   reference = truth)
  accu <- confusion.mat$overall[["Accuracy"]]
  sen <- confusion.mat$byClass[["Sensitivity"]]
  spe <- confusion.mat$byClass[["Specificity"]]
  learning.results[10*i - 1, ] <- c(paste("comb. minus", studies.names[i], 
                                                      sep = " "),
                                            studies.names[i],
                                            "random forest",
                                            signif(accu, digits = 4), 
                                            signif(sen, digits = 4), 
                                            signif(spe, digits = 4))
  
  # Supporting Vector Machine
  supp.vec <- svm(Cluster~., data = learning.set, type = "C-classification")
  pred <- predict(supp.vec, validation.set)
  confusion.mat <- confusionMatrix(data = pred, 
                                   reference = truth)
  accu <- confusion.mat$overall[["Accuracy"]]
  sen <- confusion.mat$byClass[["Sensitivity"]]
  spe <- confusion.mat$byClass[["Specificity"]]
  learning.results[10*i, ] <- c(paste("comb. minus", studies.names[i], 
                                                      sep = " "),
                                            studies.names[i],
                                            "SVM",
                                            signif(accu, digits = 4), 
                                            signif(sen, digits = 4), 
                                            signif(spe, digits = 4))
  
}

print(learning.results, n = 2*length(studies)^2)
```
#### Visualization of results
We draw a couple ROC curves based on the comparison of learning results above.
``` {r ROC curve}
# Convert characters to doubles
roc.data <- learning.results
roc.data$accuracy <- as.double(roc.data$accuracy) 
roc.data$sensitivity <- as.double(roc.data$sensitivity)
roc.data$specificity <- as.double(roc.data$specificity)

# Add a column for learning set
learning.type <- c(rep("single", 2*(length(studies) - 1)), rep("combined", 2))
learning.type <- rep(learning.type, 5)
learning.type <- t(t(learning.type))
learning.type <- factor(learning.type, levels = c("single", "combined"))
roc.data <- add_column(roc.data, Type = as.vector(learning.type))

# Add a column for false positive 
roc.data <- add_column(roc.data, false.positive = rep(1, length(roc.data$specificity)))
roc.data$false.positive <- roc.data$false.positive - roc.data$specificity

# Separate validation set type
is.Moffitt <- roc.data$validation.set == "Moffitt Arrays"
roc.data <- add_column(roc.data, validation.set.type = rep("NGS", length(roc.data$specificity)))
roc.data$validation.set.type[is.Moffitt] <- "microarrays"
roc.data$validation.set.type <- factor(roc.data$validation.set.type, levels = c("NGS", "microarrays"))

# Plot the roc curves
ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set, shape = method)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Methods") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid( . ~ method) +
  scale_color_discrete(name = "Validation Dataset") +
  scale_shape_discrete(name = "Learning Method")

ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Types of Training Sets") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid( . ~ Type) +
  scale_color_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = false.positive, y = sensitivity, col = validation.set)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) + 
  ggtitle("ROC Curve For Learning Results", subtitle = "Comparison Between Learning Methods Types of Studies of Validation Sets") + 
  xlab("1 - Specificity") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid( Type ~ validation.set.type) +
  scale_color_discrete(name = "Validation Dataset")

# Plot out the accuracy
ggplot(data = roc.data, aes(x = Type, y = accuracy, fill = validation.set)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Training Sets") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid( . ~ validation.set) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 15)) +
  scale_fill_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = Type, y = accuracy, fill = validation.set)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Training Sets And Learning Methods") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid(method ~ validation.set) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 15)) +
  scale_fill_discrete(name = "Validation Dataset")

ggplot(data = roc.data, aes(x = validation.set.type, y = accuracy, fill = validation.set.type)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter() +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "Between Types of Studies of Validation Sets") +
  xlab("Learning Method") +
  ylab("Accuracy") +
  theme_classic() + facet_grid( . ~ Type) +
  theme(axis.text.x = element_text(vjust = 0.5)) +
  scale_fill_discrete(name = "Type of Studies")
```
