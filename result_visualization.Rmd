---
title: "Machine Learning Performances Visualization"
author: "Tianyi Liu"
date: "1/16/2019"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
library(pROC)
```

```{r load_data, include = FALSE}
data_path <- "/Users/gr8lawrence/Desktop/Senior Honors Thesis/R_output_from_cluster/"

fr <- list()

load(paste(data_path, "learning_result_RT_rf.Rdata", sep = ""))
fr[[1]] <- final_results

load(paste(data_path, "learning_result_RT_svm.Rdata", sep = ""))
fr[[2]] <- final_results

load(paste(data_path, "learning_result_TSPs_rf_tp_100.Rdata", sep = ""))
fr[[3]] <- final_results

load(paste(data_path, "learning_result_TSPs_svm_tp_500.Rdata", sep = ""))
fr[[4]] <- final_results
```

## Overview

In this document, we present several visualizations of our results that demonstrate the performances of various combinations of data processing and machine learning models. 

### Data Processing

In this section, we process our data and combine everything in a giant tibble file so that downstream tasks of plotting the data would be a lot easier. We have already loaded the data from runs on the clusters and only need to rearrange them now. The `tibble` below presents the final arrangement.

```{r data_processing, echo = FALSE}
results_tibble_list <- list()
results_tibble <- tibble() # This is the final giant tibble

for (i in c(2, 4)) {
  fr[[i]]$results <- fr[[i]]$results[ ,-c(3, 4)]
  colnames(fr[[i]]$results)[1:2] <- c("training.set", "test.set")
  fr[[i]]$results <- fr[[i]]$results %>% add_column(strategy = "Single") 
  for (j in 1:dim(fr[[i]]$results)[1]) {
    if (substr(fr[[i]]$results$training.set[j], 1, 4) == "comb" || substr(fr[[i]]$results$training.set[j], 1, 4) == "Comb" ) {
      fr[[i]]$results$training.set[j] <- "Combined"
      fr[[i]]$results$strategy[j] <- "Combined"
    } 
  }
  if (i == 2) {
    results_tibble_list[[i]] <- fr[[i]]$results %>% mutate(method = "SVM", transformation = "Rank")
  } else {
    results_tibble_list[[i]] <- fr[[i]]$results %>% mutate(method = "SVM", transformation = "TSPs")
  }
}


for (i in c(1, 3)) {
  colnames(fr[[i]]$results)[1:2] <- c("training.set", "test.set")
  fr[[i]]$results <- fr[[i]]$results %>% add_column(strategy = "Single") 
  for (j in 1:dim(fr[[i]]$results)[1]) {
    if (substr(fr[[i]]$results$training.set[j], 1, 4) == "comb" || substr(fr[[i]]$results$training.set[j], 1, 4) == "Comb" ) {
      fr[[i]]$results$training.set[j] <- "Combined"
      fr[[i]]$results$strategy[j] <- "Combined"
    } 
  }
  if (i == 1) {
    results_tibble_list[[i]] <- fr[[i]]$results %>% mutate(method = "RF", transformation = "Rank")
  } else {
    results_tibble_list[[i]] <- fr[[i]]$results %>% mutate(method = "RF", transformation = "TSPs")
  }
}


results_tibble <- rbind(results_tibble_list[[1]], results_tibble_list[[2]], results_tibble_list[[3]], results_tibble_list[[4]])

results_tibble <- results_tibble[order(results_tibble$transformation), ]  
results_tibble$accuracy <- as.numeric(results_tibble$accuracy)
results_tibble$sensitivity <- as.numeric(results_tibble$sensitivity)
results_tibble$specificity <- as.numeric(results_tibble$specificity)
results_tibble$method <- factor(results_tibble$method, levels = c("PLR", "RF", "SVM", "MTL"))
results_tibble$transformation <- factor(results_tibble$transformation, levels = c("Rank", "TSPs"))
results_tibble$strategy <- factor(results_tibble$strategy, levels = c("Single", "Combined"))

## We average the measurements of performance across results from "Single" dataset learning.
summary_table <- aggregate(cbind(accuracy, sensitivity, specificity) ~ test.set + method + transformation + strategy, data = results_tibble, mean)

print(as.tibble(summary_table, n = 10))
```

### Prediction Accuracy

One measurement of performances for any machine learning program is the prediction accuracy. Here, we present the boxplots for the accuracies of several of our combinations of data transformation methods, learning methods, and  learning strategies.

#### Figure 1 

```{r pred_accu}
ggplot(data = summary_table, aes(x = transformation, y = accuracy, fill = transformation)) +
  geom_boxplot(alpha = 0.8, width = 0.2) +
  ggtitle("Comparison of Prediction Accuracy") +
  xlab("Transformation") +
  ylab("Accuracy") +
  theme_light() + facet_grid(strategy ~ method) +
  theme(axis.text.x = element_text(vjust = 0.5)) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9"), 
                    name = "Transformation")
```

#### Supplementary Figures
```{r pred_accu_method, echo = FALSE}
ggplot(data = results_tibble, aes(x = method, y = accuracy, fill = method)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "by learning method") +
  xlab("Method") +
  ylab("Accuracy") +
  theme_classic() +
  scale_fill_discrete(name = "Method")
```

```{r pred_accu_transoformation, echo = FALSE}
ggplot(data = results_tibble, aes(x = transformation, y = accuracy, fill = transformation)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "by transformation method") +
  xlab("Transformation") +
  ylab("Accuracy") +
  theme_classic() +
  scale_fill_discrete(name = "Transformation")
```

```{r pred_accu_strategy, echo = FALSE}
ggplot(data = results_tibble, aes(x = strategy, y = accuracy, fill = strategy)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Accuracy", subtitle = "by learning strategy") +
  xlab("Strategy") +
  ylab("Accuracy") +
  theme_classic() +
  scale_fill_discrete(name = "Strategy")
```

### Prediction Sensitivity

```{r pred_sensitvity, echo = FALSE}
ggplot(data = results_tibble, aes(x = interaction(transformation, strategy, sep = "-"), y = accuracy, col = interaction(transformation, strategy, sep = "-"))) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Senstivity") +
  xlab("Tranformation-Strategy") +
  ylab("Sensitivity") +
  theme_classic() + facet_grid(. ~ method) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 90)) + 
  scale_color_discrete(name = "Trans/Strategy")
```

```{r pred_sensitvity_method, echo = FALSE}
ggplot(data = results_tibble, aes(x = method, y = sensitivity, fill = method)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Senstivity", subtitle = "by learning method") +
  xlab("Method") +
  ylab("Sensitivity") +
  theme_classic() + 
  scale_fill_discrete(name = "Method") 
```

```{r pred_sensitvity_transformation, echo = FALSE}
ggplot(data = results_tibble, aes(x = transformation, y = sensitivity, fill = transformation)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Senstivity", subtitle = "by transformation method") +
  xlab("Transformation Method") +
  ylab("Sensitivity") +
  theme_classic() + 
  scale_fill_discrete(name = "Method") 
```


```{r pred_sensitvity_strategy, echo = FALSE}
ggplot(data = results_tibble, aes(x = strategy, y = sensitivity, fill = strategy)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Senstivity", subtitle = "by learning strategy") +
  xlab("Learning Strategy") +
  ylab("Sensitivity") +
  theme_classic() + 
  scale_fill_discrete(name = "Strategy") 
```

### Prediction Specificity

```{r pred_specificity, echo = FALSE}
ggplot(data = results_tibble, aes(x = interaction(transformation, strategy, sep = '-'), y = specificity, fill = interaction(transformation, strategy, sep = '-'))) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Specificity") +
  xlab("Transformation/Strategy") +
  ylab("Specificity") +
  theme_classic() + facet_grid(. ~ method) +
  theme(axis.text.x = element_text(vjust = 0.5, angle = 90)) + 
  scale_fill_discrete(name = "Trans/Strategy") 
```

```{r pred_specificity_method, echo = FALSE}
ggplot(data = results_tibble, aes(x = method, y = specificity, fill = method)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Specificity", subtitle = "by learning method") +
  xlab("Method") +
  ylab("Specificity") +
  theme_classic() +
  scale_fill_discrete(name = "Method") 
```

```{r pred_specificity_transformation, echo = FALSE}
ggplot(data = results_tibble, aes(x = transformation, y = specificity, fill = transformation)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Specificity", subtitle = "by transformation method") +
  xlab("Transformation Method") +
  ylab("Specificity") +
  theme_classic() +
  scale_fill_discrete(name = "Transformation Method") 
```

```{r pred_specificity_strategy, echo = FALSE}
ggplot(data = results_tibble, aes(x = strategy, y = specificity, fill = strategy)) +
  geom_boxplot(alpha = 0.8, width = 0.25) +
  ggtitle("Comparison of Prediction Specificity", subtitle = "by learning strategy") +
  xlab("Learning Strategy") +
  ylab("Specificity") +
  theme_classic() +
  scale_fill_manual(values = c("#E69F00", "#56B4E9"), name = "Learning Strategy")
```


### ROC Plot

Another way to visualize the performance of each program is to draw ROC curves. Because we know from the accuracy plot that random forest plus TSPs produces the best prediction accuracy, we will plot out the roc curves only for this combination under each learning cycle (from different datasets). The ROC curves are plotted using the `pROC` package. 

#### Preprocessing

We need to obtain the truth vectors before proceeding. 

``` {r data_pp2}
dataPath <- "/Users/gr8lawrence/Desktop/Senior Honors Thesis/datasets/" # The name of the directory where all data sets are stored locally
studies.names <- c("Aguirre-Seq", "Linehan-Seq", "COMPASS", "Moffitt Arrays", "TCGA-PAAD") # Names of the studies in the same order as being loaded
# Load datasets
load(paste(dataPath, "Aguirre_seq_plus.RData", sep = ""))
load(paste(dataPath, "Linehan_Seq_plus.RData", sep = ""))
load(paste(dataPath, "COMPASS.2017_plus.RData", sep = ""))
load(paste(dataPath, "Moffitt_GEO_array_plus.RData", sep = ""))
load(paste(dataPath, "TCGA_PAAD_plus.RData", sep = ""))
# Incorporate all datasets into a single list object
studies.df <- list(Aguirre_seq_plus, Linehan_Seq_plus, COMPASS.2017_plus, Moffitt_GEO_array_plus, TCGA_PAAD_plus)
num.studies <- length(studies.df) # Number of datasets

## Assign a truth vector to each cycle of learning using the loop below
truth.list <- list() # A list object to hold all truth vectors
for (i in 1:num.studies) {
  truth <- studies.df[[i]]$sampInfo$cluster.MT[!is.na(studies.df[[i]]$sampInfo$cluster.MT)] 
  for (j in 1:num.studies) {
    truth.list[[(i - 1) * num.studies + j]] <- truth
  }
}

truth.list[[1]] # Print out the first vector as an example
```
#### Plotting

```{r roc_curve, echo = FALSE}
print("Placeholder for ROC curves")


```










